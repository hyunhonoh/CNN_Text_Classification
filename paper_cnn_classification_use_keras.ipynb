{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Text Classification\n",
    "\n",
    "- reference\n",
    "    - https://ratsgo.github.io/natural%20language%20processing/2017/03/19/CNN/\n",
    "    - https://github.com/bhaveshoswal/CNN-text-classification-keras\n",
    "    - https://cloud.google.com/blog/big-data/2017/10/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "def make_input(documents):\n",
    "    max_document_length = 100\n",
    "    # tensorflow.contrib.learn.preprocessing 내에 VocabularyProcessor라는 클래스를 이용\n",
    "    # 모든 문서에 등장하는 단어들에 인덱스를 할당\n",
    "    # 길이가 다른 문서를 max_document_length로 맞춰주는 역할\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)  # 객체 선언\n",
    "    x = np.array(list(vocab_processor.fit_transform(documents)))\n",
    "    ### 텐서플로우 vocabulary processor\n",
    "    # Extract word:id mapping from the object.\n",
    "    # word to ix 와 유사\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    # Sort the vocabulary dictionary on the basis of values(id).\n",
    "    sorted_vocab = sorted(vocab_dict.items(), key=lambda x: x[1])\n",
    "    # Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "    # word with id i goes at index i of the list.\n",
    "    vocabulary = list(list(zip(*sorted_vocab))[0])\n",
    "    return x, vocabulary, len(vocab_processor.vocabulary_)\n",
    "\n",
    "def make_output(category):\n",
    "    classes = pd.read_csv('./paper_class', header=None,)\n",
    "    one_hot_vectors = np.eye(len(classes), dtype=int)\n",
    "    class_vectors = {}\n",
    "    y = []\n",
    "    for i, cls in enumerate(list(classes[0])):\n",
    "        class_vectors[cls] = one_hot_vectors[i]\n",
    "    for c in category:\n",
    "        y.append(class_vectors[c])\n",
    "    return np.array(y)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    cnn = pd.read_excel('./paper_subject_nlp.xlsx')\n",
    "    contents = cnn['subject_nlp']\n",
    "    category = cnn['paper_category']\n",
    "\n",
    "    x, vocabulary, vocabulary_size = make_input(contents)\n",
    "\n",
    "    y = make_output(category)\n",
    "    return x, y, vocabulary, vocabulary_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading data')\n",
    "\n",
    "x, y, vocabulary, vocabulary_size = load_data()\n",
    "print(\"x shape {}\".format(x.shape))\n",
    "print(\"vocabulary_size {}\".format(vocabulary_size))\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_dict = {k: v for v, k in enumerate(vocabulary)}\n",
    "# Save\n",
    "np.save('vocabulary_dict.npy', vocabulary_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = x.shape[1]\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 30\n",
    "\n",
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=10, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Traning Model...\")\n",
    "early_stopping = EarlyStopping()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint, early_stopping], validation_data=(X_test, y_test))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
